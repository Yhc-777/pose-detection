#!/usr/bin/env python3
"""
Convert PyTorch models to ONNX format for C++ inference
"""

import torch
import sys
import os
import warnings

# æŠ‘åˆ¶ONNXå¯¼å‡ºè­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

sys.path.append('src')

from src.models.fall_detection_gru import FallDetectionGRU
from ultralytics import YOLO

def convert_gru_model():
    """Convert GRU model to ONNX"""
    print("Converting GRU model to ONNX...")
    
    try:
        # Load trained model
        model = FallDetectionGRU(
            input_size=34,
            hidden_size=64,
            num_layers=2,
            output_size=3,
            dropout_prob=0.6
        )
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_path = 'models/gru_model.pth'
        if not os.path.exists(model_path):
            print(f"Error: Model file not found: {model_path}")
            return False
        
        model.load_state_dict(torch.load(model_path, map_location='cpu'))
        model.eval()
        
        # åˆ›å»ºdummy input - æ‰¹æ¬¡å¤§å°è®¾ä¸º1ä»¥é¿å…è­¦å‘Š
        dummy_input = torch.randn(1, 20, 34)  # (batch_size=1, sequence_length, input_size)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs('models', exist_ok=True)
        
        # Export to ONNX with more specific settings
        torch.onnx.export(
            model,
            dummy_input,
            'models/gru_model.onnx',
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input'],
            output_names=['output'],
            dynamic_axes={
                'input': {0: 'batch_size'},  # å…è®¸åŠ¨æ€æ‰¹æ¬¡å¤§å°
                'output': {0: 'batch_size'}
            },
            verbose=False
        )
        
        print("âœ“ GRU model converted to: models/gru_model.onnx")
        
        # éªŒè¯ONNXæ¨¡å‹
        try:
            import onnx
            onnx_model = onnx.load('models/gru_model.onnx')
            onnx.checker.check_model(onnx_model)
            print("âœ“ ONNX model validation passed")
        except Exception as e:
            print(f"Warning: ONNX model validation failed: {e}")
        
        return True
        
    except Exception as e:
        print(f"Error converting GRU model: {e}")
        return False

def convert_yolo_model():
    """Convert YOLO model to ONNX"""
    print("Converting YOLO model to ONNX...")
    
    try:
        # æ£€æŸ¥YOLOæ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        yolo_model_path = 'models/yolo11n-pose.pt'
        if not os.path.exists(yolo_model_path):
            print(f"Error: YOLO model file not found: {yolo_model_path}")
            print("Please download the model first:")
            print("wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolo11n-pose.pt -O models/yolo11n-pose.pt")
            return False
        
        # Load YOLO model
        model = YOLO(yolo_model_path)
        
        # Export to ONNX
        success = model.export(
            format='onnx',
            imgsz=640,
            optimize=True,
            half=False,
            int8=False,
            dynamic=False,
            simplify=True,
            verbose=False
        )
        
        if success:
            # æ£€æŸ¥å¯¼å‡ºçš„æ–‡ä»¶ä½ç½®
            exported_file = yolo_model_path.replace('.pt', '.onnx')
            target_file = 'models/yolo11n-pose.onnx'
            
            if os.path.exists(exported_file) and exported_file != target_file:
                os.rename(exported_file, target_file)
            
            print("âœ“ YOLO model converted to: models/yolo11n-pose.onnx")
            return True
        else:
            print("âœ— YOLO model conversion failed")
            return False
            
    except Exception as e:
        print(f"Error converting YOLO model: {e}")
        return False

def verify_models():
    """Verify that the converted models exist and are valid"""
    print("\nVerifying converted models...")
    
    models_to_check = [
        'models/gru_model.onnx',
        'models/yolo11n-pose.onnx'
    ]
    
    all_good = True
    for model_path in models_to_check:
        if os.path.exists(model_path):
            size_mb = os.path.getsize(model_path) / (1024 * 1024)
            print(f"âœ“ {model_path} ({size_mb:.1f} MB)")
        else:
            print(f"âœ— {model_path} - Not found")
            all_good = False
    
    return all_good

if __name__ == "__main__":
    print("PyTorch to ONNX Model Converter")
    print("=" * 40)
    
    # æ£€æŸ¥å¿…è¦çš„åŒ…
    try:
        import onnx
        import onnxruntime
        print("âœ“ ONNX packages available")
    except ImportError as e:
        print(f"âœ— Missing required packages: {e}")
        print("Please install: pip install onnx onnxruntime")
        sys.exit(1)
    
    # Create models directory
    os.makedirs('models', exist_ok=True)
    
    # Convert models
    gru_success = convert_gru_model()
    yolo_success = convert_yolo_model()
    
    # Verify results
    if gru_success and yolo_success:
        if verify_models():
            print("\nğŸ‰ All models converted successfully!")
            print("\nNext steps:")
            print("1. Install C++ dependencies: ./install_dependencies.sh")
            print("2. Build the project: ./build.sh")
            print("3. Run inference: ./build/activity_detection input.mp4 --output result.mp4")
        else:
            print("\nâš ï¸  Some models may not have been converted properly")
    else:
        print("\nâŒ Model conversion failed")
        if not gru_success:
            print("- GRU model conversion failed")
        if not yolo_success:
            print("- YOLO model conversion failed")
